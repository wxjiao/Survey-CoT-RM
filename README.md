# Survey of Chain-of-Thought Reward Model

This is a collection of research papers on __Chain-of-Thought Reward Model__ (CoT RM).

Recent advances in industrial open-source LLMs suggest that CoT RM is more favorable than Classic RM.

:star:The main advantages include:
- Less risk of reward hacking
- Interpretable reward signal
- Higher accuracy

> DeepSeek-V3
![image](https://github.com/user-attachments/assets/5d2881df-055a-46cf-b6fa-cbd1d32932f5)

> Kimi-k1.5
![image](https://github.com/user-attachments/assets/c1cc4c45-7e10-4701-908f-225e9233924a)


## ðŸ“– Papers  

| Title | Publication Date | Link |
|---------------------------------|------------------------|---------------------------------|
| Critique-out-Loud Reward Models | 21 Aug 2024 | [Arxiv](https://arxiv.org/abs/2408.11791) |
| Self-Generated Critiques Boost Reward Modeling for Language Models | 25 Nov 2024 | [NAACL](https://arxiv.org/abs/2411.16646) |
| Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge | 30 Jan 2025 | [Arxiv](https://arxiv.org/abs/2501.18099)|
| Improving Reward Models with Synthetic Critiques | 31 May 2024 | [Arxiv](https://arxiv.org/abs/2405.20850) |
| PairJudge RM: Perform Best-of-N Sampling with Knockout Tournament | 22 Jan 2025 | [Arxiv](https://arxiv.org/abs/2501.13007)|
| Generative Verifiers: Reward Modeling as Next-Token Prediction | 27 Aug 2024 | [ICLR](https://arxiv.org/abs/2408.15240)|
| Beyond Scalar Reward Model: Learning Generative Judge from Preference Data | 1 Oct 2024 | [Arxiv](https://arxiv.org/abs/2410.03742v2) |


