# Survey of Chain-of-Thought Reward Model

This is a collection of research papers on *Chain-of-Thought Reward Model (CoT RM)*.

Recent advances in industrial open-source LLMs suggest that CoT RM is more favorable than Classic RM.
The main advantages include:
- Less risk of reward hacking
- Interpretable reward signal
- Higher accuracy

---

## â­ Quote

**DeepSeek-V3**
>![image](https://github.com/user-attachments/assets/5d2881df-055a-46cf-b6fa-cbd1d32932f5)

**Kimi-k1.5**
>![image](https://github.com/user-attachments/assets/c1cc4c45-7e10-4701-908f-225e9233924a)



## ğŸ“– Papers  

| Title | Publication Date | Link |
|---------------------------------|------------------------|---------------------------------|
| Inference-Time Scaling for Generalist Reward Modeling | 03 Apr 2025 | [ArXiv](https://arxiv.org/abs/2504.02495) |
| Scaling Evaluation-time Compute with Reasoning Models as Process Evaluators | 25 Mar 2025 | [Arxiv](https://arxiv.org/abs/2503.19877) |
| Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge | 30 Jan 2025 | [Arxiv](https://arxiv.org/abs/2501.18099)|
| PairJudge RM: Perform Best-of-N Sampling with Knockout Tournament | 22 Jan 2025 | [Arxiv](https://arxiv.org/abs/2501.13007)|
| Self-Generated Critiques Boost Reward Modeling for Language Models | 25 Nov 2024 | [NAACL](https://arxiv.org/abs/2411.16646) |
| Beyond Scalar Reward Model: Learning Generative Judge from Preference Data | 01 Oct 2024 | [Arxiv](https://arxiv.org/abs/2410.03742v2) |
| Generative Verifiers: Reward Modeling as Next-Token Prediction | 27 Aug 2024 | [ICLR](https://arxiv.org/abs/2408.15240)|
| Critique-out-Loud Reward Models | 21 Aug 2024 | [Arxiv](https://arxiv.org/abs/2408.11791) |
| Improving Reward Models with Synthetic Critiques | 31 May 2024 | [Arxiv](https://arxiv.org/abs/2405.20850) |


## Notes

### **Scaling Evaluation-time Compute with Reasoning Models as Process Evaluators**
é“¾æ¥ï¼šhttps://arxiv.org/abs/2503.19877 <br>
æ—¥æœŸï¼š2025å¹´3æœˆ25æ—¥ <br>
å•ä½ï¼šCMUç­‰ <br>
ç®€ä»‹ï¼šæ ¸å¿ƒæ˜¯æ¯”è¾ƒå…¨é¢åœ°åˆ†æäº†åœ¨è¯„ä»·é˜¶æ®µï¼Œé‡‡ç”¨long CoTæ¨ç†èƒ½åŠ›å¯¹äºè¯„ä»·å‡†ç¡®æ€§çš„å½±å“ã€‚è¿™é‡Œçš„scalingä¸»è¦æ˜¯é€šè¿‡ï¼ˆ1ï¼‰å°†ç­”æ¡ˆæ‹†åˆ†æˆNä¸ªstepï¼Œå¹¶å¯¹æ¯ä¸ªstepè¿›è¡Œlong CoTè¯„ä»·ï¼›ï¼ˆ2ï¼‰ç­”æ¡ˆæ•´ä½“ä¹Ÿä¼šåšä¸€æ¬¡long CoTè¯„ä»·ã€‚æœ¬è´¨æ˜¯åœ¨long CoTåŸºç¡€ä¸ŠåŒæ—¶é›†æˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹(PRM)å’Œç»“æœå¥–åŠ±æ¨¡å‹(ORM)çš„ç­–ç•¥ã€‚ORMå¾—åˆ†é‡‡ç”¨è¾“å‡ºä½ç½®ä¸Šâ€1â€å’Œâ€œ0â€çš„logitsçš„äºŒå…ƒsoftmaxï¼ŒPRMæ¯ä¸ªstepå¾—åˆ†è®¡ç®—æ–¹å¼ç±»ä¼¼ï¼Œä½†æ˜¯PRMæ€»åˆ†é‡‡ç”¨mean_logitsã€‚æœ€åæ•´ä½“åˆ†æ•°é‡‡ç”¨ $\alpha \rm{ORM} + (1-\alpha)\rm{PRM}$ çš„åŠ æƒå½¢å¼ï¼Œ$\alpha$ é»˜è®¤0.5ã€‚

<img width="860" alt="image" src="https://github.com/user-attachments/assets/bd55c2bd-1a51-4ed8-959f-4083f8c96145" />


